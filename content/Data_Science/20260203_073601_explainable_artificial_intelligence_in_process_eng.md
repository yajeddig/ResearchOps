---
title: "eXplainable Artificial Intelligence in Process Engineering: Promises, Facts, and Current Limitations"
date: 2026-02-03
category: Data_Science
confidence: 0.98
tags: ['Explainable AI', 'XAI', 'Process Engineering', 'Machine Learning', 'Deep Learning', 'Systematic Literature Review', 'Industrial AI', 'Process Optimization', 'Fault Detection', 'Quality Control', 'Model Interpretability', 'AI Ethics', 'Decision Support Systems']
source: "https://mdpi-res.com/d_attachment/asi/asi-07-00121/article_deploy/asi-07-00121-v2.pdf?version=1733210812"
type: Article
source_type: Paper
hash: 073601
---

## üéØ Relevance
This paper is highly useful for industrial data scientists and process engineers, as it provides a comprehensive overview of eXplainable Artificial Intelligence (XAI) methodologies and their current state of adoption and limitations within process engineering. It highlights the critical need for XAI to build trust, ensure ethical AI deployment, and enable better decision-making in complex industrial processes like optimization, fault detection, and quality control. Understanding XAI can lead to higher ROI by improving the reliability and transparency of AI-driven solutions, fostering operator acceptance, and addressing regulatory requirements for trustworthy AI.

## üìñ Content
Citation: Di Bonito, L.P.; Campanile, L.; Di Natale, F.; Mastroianni, M.; Iacono, M. eXplainable Artificial Intelligence in Process Engineering: Promises, Facts, and Current Limitations. Appl. Syst. Innov. 2024 , 7,121. https://doi.org/10.3390/ asi7060121 Received: 15 October 2024 Revised: 14 November 2024 Accepted: 22 November 2024 Published: 30 November 2024                

Copyright: ¬© 2024 by the authors. Published by MDPI on behalf of the International Institute of Knowledge Innovation and Invention. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/).

Systematic Review 

# eXplainable Artificial Intelligence in Process Engineering: Promises, Facts, and Current Limitations 

Luigi Piero Di Bonito 1,‚Ä†  , Lelio Campanile 1, *,‚Ä†  , Francesco Di Natale 2,‚Ä†  , Michele Mastroianni 3,‚Ä† 

and Mauro Iacono 1,‚Ä†  

1 Dipartimento di Matematica e Fisica, Universit√† degli Studi della Campania ‚ÄúLuigi Vanvitelli‚Äù, 81100 Caserta, Italy; luigipiero.dibonito@unicampania.it (L.P.D.B.); mauro.iacono@unicampania.it (M.I.)  

2 Dipartimento di Ingegneria Chimica, dei Materiali e della Produzione Industriale, Universit√† degli Studi di Napoli ‚ÄúFederico II‚Äù, 80125 Napoli, Italy; francesco.dinatale@unina.it  

3 Dipartimento di Scienze Agrarie, Alimenti, Risorse Naturali e Ingegneria, Universit√† degli Studi di Foggia, 84084 Fisciano, Italy; michele.mastroianni@unifg.it 

* Correspondence: lelio.campanile@unicampania.it  

‚Ä† These authors contributed equally to this work. 

Abstract: Artificial Intelligence (AI) has been swiftly incorporated into the industry to become a part of both customer services and manufacturing operations. To effectively address the ethical issues now being examined by the government, AI models must be explainable in order to be used in both scientific and societal contexts. The current state of eXplainable artificial intelligence (XAI) in process engineering is examined in this study through a systematic literature review (SLR), with particular attention paid to the technology‚Äôs effect, degree of adoption, and potential to improve process and product quality. Due to restricted access to sizable, reliable datasets, XAI research in process engineering is still primarily exploratory or propositional, despite noteworthy applicability in well-known case studies. According to our research, XAI is becoming more and more positioned as a tool for decision support, with a focus on robustness and dependability in process optimization, maintenance, and quality assurance. This study, however, emphasizes that the use of XAI in process engineering is still in its early stages, and there is significant potential for methodological development and wider use across technical domains. 

Keywords: explainable artificial intelligence; process engineering; systematic literature review (SLR); industrial AI adoption 

1. Introduction 

Adopting Artificial Intelligence (AI) techniques is largely advocated or affirmed as a good practice for pursuing innovation through digitalizing industrial processes [ 1]. Industrial processes are characterized by an elevated degree of automation, with a rising number of indicators and controllers and an increasing complexity of the Distributed Control Systems (DCS). The acquisition of large amounts of experimental data provides large datasets, which can be suitably used for statistical analysis as well as for the application of AI processes [ 2]. The main application of AI is related to off-line diagnosis, anomaly detection, process control, and quality prediction. The size of the dataset and the nature of the process make the explainability of AI pivotal for any organization interested in accelerating their control of process functioning, the assessment of predictive maintenance routines or quality control [ 3‚Äì 6]. Additionally, the European Union indicated innovation in sustainability, environmental problems and eXplainable Artificial Intelligence (XAI) among its key strategic directions. The application of XAI to the process industry is still at a pioneering level, both because datasets are scarce, heavily biased, and sparsely distributed and because of the intrinsic novelty of this discipline. Despite the abundance of experimental data collected 

Appl. Syst. Innov. 2024 , 7, 121. https://doi.org/10.3390/asi7060121 https://www.mdpi.com/journal/asi Appl. Syst. Innov. 2024 , 7, 121 2 of 28 

by companies, most of the studies refer to simulated data and metadata because real data are very rarely shared [ 7‚Äì 9]. Nevertheless, part of the literature claims that XAI is an established practice in the field and both academia and industry strive to increase its adoption and exploitation. Understanding the actual results and directions requires filtering away marketing noise (and also a fuzzy use of keywords in scientific literature) and focusing on the solutions and the techniques made available to a wide audience through publication in the open, peer-reviewed literature. To clarify the state of the art and to provide a sound foundation for future research programs, we joined the efforts of a computer science and a process engineering research team to explore the literature through a set of research questions aimed to address the following: (i) Depict a clearer image of the existing results; (ii) Understand the influence of XAI in computer science; (iii) Verify how much the terms are coherently used between computer science and process engineering; (iv) Exploit the main objectives, the impacts, and benefits that XAI and its components are expected to provide to process engineering, either in terms of process and product performance improvement. We found that XAI models are still at an exploratory stage, with several examples based on third-party or simulated data rather than on full-scale processes. The scrutiny of selected works shows a low yield of papers pertinent to the subject of this work with regard to those found from the research query. Other works previously known to the authors have not been found by the research queries despite being available in the same database: this signals a non-standard use of keywords for this subject. Additionally, we found that the main goal of XAI models is related to assuring transparent and robust descriptions of process functioning and fault analysis. 

2. Terms and Scope of This Research 

At an industrial level, process engineering is the knowledge and application of the fundamental principles and laws of nature that allow people to turn raw materials and energy into products that are important to humans. Process engineering encompasses a wide range of subsidiary topics such as mechanical-, aerospace-, computer-, thermal-, bio-, electrochemical-, chemical-, and systems-process engineering, as well as nanotechnology. For example, chemical process engineers can synthesize and purify vast amounts of desired chemical products to produce intermediates for mechanical processing, which produce components for electronics, aerospace, or other applications. Oil and gas, automotive, chemical, paper production, metal, cement, material development, mining, pharmaceutical, civil engineering, packaging, and food and beverage sectors are just a few examples [ 10 ‚Äì14 ]. In the last number of years, process engineering has experienced profound modifications derived from the improvement of logistics, the modification of value chains, and, above all, a greater demand for high-performance products and the establishment of economic and social models based on sustainable development principles [15‚Äì17]. As a result, several national and international administrations and institutions have introduced innovative/sustainable manufacturing development plans in recent years. The most celebrated action in this sense is the establishment of the Sustainable Development Goals proposed by the UN, which poses targets for the improvement, within 2030, of several social indicators, such as access to clean water and sanitation, affordable and clean energy, responsible consumption and production, improved quality of human life and the environment [18‚Äì20]. Integration between industrial manufacturing and Information and Communication Technologies (ICT) is expected to play a key role in the transition from the traditional to the new production model [ 21 ‚Äì24 ]. Process systems engineering is the application of systematic computer-based approaches to model process engineering systems. The multiscale nature of many process engineering applications and the complexities of systems layout and interconnecting systems result in mathematical models characterized by extremely high dimensionality and very complicated correlations between different parameters and several optimization tools, and data reduction methods are used to ensure suitable management and control of the process at hand [ 25 ‚Äì 29 ]. Due to the widespread Appl. Syst. Innov. 2024 , 7, 121 3 of 28 

usage of distributed control systems over the last few decades, the process industry is collecting a vast and ever-increasing amount of data [ 30 ]. While developing first-principles models for more complex processes becomes increasingly challenging, data-driven process modeling, monitoring, prediction, and control have become a suitable approach to support process management [ 31 ,32 ]. The demand for automated techniques to model complex systems is growing [ 33 ,34 ]. This demand has been answered by the development of AI, first in the form of Machine Learning (ML) algorithms and later with the introduction of Deep Learning (DL) algorithms. While AI has been widely used in ICT and smart grids, there are still numerous problems with implementing AI for process engineering because of a lack of model explainability, transparency, and trustworthiness. To resolve these issues, XAI algorithms have been defined to support decision-makers in the comprehension of the model outputs by opening the AI ‚Äúblack boxes‚Äù [ 35 ,36 ]. By increasing model explainability and transparency, XAI could also promote the creation of more accurate and advanced AI models. The goal of this paper is to understand the current maturity of use and diffusion of XAI in process engineering, as well as to leverage literature to identify potential weaknesses and limitations of the methodologies that may affect the progress and outcome of our research and industrial activities, also classifying the application areas (e.g., chemical, mechanical, aerospace, or bioengineering) in which adoption is wider and potential community support is stronger. The analysis includes, as a side effect, an investigation into the type of ML or DL algorithms employed, as well as the type and source of the dataset used during the training step. 

3. Theoretical Framework on eXplainable Artificial Intelligence 

AI is a broad term that includes the theory and use of computer systems to mimic tasks that often call for human intelligence, such as strategic thinking or the recognition of categorization schemes, like speech or picture recognition. Most of the time, these issues cannot be resolved by analytical approaches. As a branch of artificial intelligence, machine learning (ML) uses statistical models and algorithms to analyze and draw conclusions from data. Furthermore, DL is a branch of ML that builds ML systems using Artificial Neural Networks (ANN). With applications in criminology, health, auto self-piloting, chemical reaction path detection, and many other fields, artificial intelligence is becoming more and more significant in our daily lives. In process engineering, the modeling of phenomena can take either a deterministic or probabilistic approach. Deterministic models, based on well-defined mathematical equations, are often simpler to implement and interpret but fail to fully capture the nonlinearities typical of real systems. On the other hand, probabilistic models include variability and uncertainties, which describe phenomena more accurately but at the cost of greater computational complexity. A probabilistic approach, supported by artificial intelligence, thus allows for a more realistic representation of fluctuations and stochastic behavior in industrial systems. Integration with Explainable AI (XAI) techniques would not only make the decision-making process in task resolution more transparent but also allow clarification of the phenomena underlying system nonlinearities. According to new European Union recommendations aiming at promoting moral, open, and trustworthy AI processes, the ethical implications of this process are extremely pertinent [ 37 ]. These recommendations emphasize the repeatability and dependability of results, thus preserving operator safety and encouraging smooth interaction between process operators and AI systems. Two fundamental ideas‚Äîthe stage of explainability and the breadth of explainability‚Äîare the foundation for the conception and construction of a trustworthy and explainable AI methodology, according to published research [36]. The process by which the model is rendered explainable is referred to as the ‚Äústage of explainability‚Äù. The literature makes a distinction between models that can be described by external XAI methods and those that can be explained by structure [ 38 ]. The difference between interpretable models and model interpretability methodologies is another way to Appl. Syst. Innov. 2024 , 7, 121 4 of 28 

conceptualize this dichotomy. The distinction between intrinsic and post-hoc explainability is a more often-used classification: 

*   While striving for optimal performance, intrinsic explainability approaches usually incorporate the rationale behind the choice from the start of the data training. These techniques are frequently employed to produce explanations for transparent models, such as Bayesian models, decision trees, fuzzy logic, linear/logistic regression, and others. Transparent models make use of some degree of interpretability on their own. In particular, new types of interpretable models arise in literature, such as interpretable cascades [ 39 ] or non iterative artificial neural network which provides interpretable results in polynomial form [ 40 ]. By definition, intrinsic methods are model-specific, which means that explainability is limited to that particular class or kind of algorithm. 
*   in addition to the underlying model, post-hoc techniques also use an external or surrogate model. When AI models do not fulfill any of the requirements to be declared transparent, a different approach must be used to explain the model‚Äôs choices. The base model stays the same, and the external model imitates the base model‚Äôs behavior to explain the users. These techniques are usually linked to models (such as tree ensembles, support vector machines, multi-layer neural networks, convolutional neural networks, recurrent neural networks, and similar) whose inference mechanism is unknown to users. Post-hoc techniques can be used with any AI model because they are often model-agnostic. In terms of model-agnostic post-hoc methodologies, several have been developed recently to improve the explainability of AI models using data science, ML, and statistics techniques [ 41 ]. Four broad categories may be used to group these techniques: example-based explanation, impact approaches, information extraction, and visualization. 
    *   Visualization: This technique uses representations of an ML model, such as a DN, which is a natural way to look at the pattern hidden inside a cell. Three well-known visualization techniques are individual conditional expectation (ICE), partial dependence plot (PDP), and surrogate models: 
        *   More complex models are described by surrogate models, which are simple models. In order to understand the latter, a trainable model‚Äîlike a decision tree or linear model‚Äîis trained using the original black-box model‚Äôs predictions; 
        *   A graphical representation known as PDP facilitates the visualization of an averaged partial relationship between one or more input variables and the predictions of a black-box model; 
        *   While PD charts provide a rough overview of a model‚Äôs activities, ICE graphs disaggregate the PDP output to show interactions and individual differences. 
    *   Knowledge extraction is the process of obtaining, in an intelligible way, the information that the algorithm records as an internal representation during training. When considering an ANN, knowledge extraction confronts the difficulty of extracting explanations from the network. Rule extraction and model distillation are two kinds of techniques for obtaining information concealed in complex algorithms: 
        *   rule extraction provides a symbolic and understandable explanation of the data the algorithm has learned during training by utilizing its input and output to extract rules that mimic the decision-making process; 
        *   model distillation is a compression technique used to transport information (dark knowledge) from deep networks (the ‚Äúteacher‚Äù) to shallow networks. 
    *   Influence methods: This approach assesses the importance or relevance of a feature by altering internal or input components and documenting the extent to which the changes affect model performance. The use of influence tactics is common. Three methods for assessing the significance of an input variable are feature importance, sensitivity analysis, and Layer-wise Relevance Propagation (LRP): Appl. Syst. Innov. 2024 , 7, 121 5 of 28 
        *   sensitivity analysis explains how the output is affected by changes in its input and/or algorithm parameters. It is frequently used to test models for stability and reliability, either as a tool to identify and eliminate irrelevant input attributes or as a foundation for a more potent explanation technique (e.g., decomposition) 
        *   LRP redistributes the prediction function backwards, backpropagating up to the input layer from the network output layer. One important component of this redistribution process is relevance conservation. 
        *   Feature importance measures how much each feature, or input variable, contributes to a complicated AI/ML model‚Äôs predictions. The rise in the model prediction error following feature permutation is used to assess a feature‚Äôs importance. As the values of important characteristics are changed, the model error rises. To maintain a consistent model error throughout permutation, the model disregards the values of unimportant attributes. 
    *   Example-based explanation: In accordance with this paradigm, the practitioner describes the behavior of AI/ML models by selecting particular examples from the dataset. Two possible example-based interpretability techniques are critiques and prototypes, as well as counterfactual justifications: 
        *   Prototypes and criticisms are a subset of typical cases drawn from the data; hence, item membership is defined by its resemblance to the prototypes, resulting in over-generalization. To overcome this, benefit exceptions, also known as criticisms, must be indicated for situations that are not properly represented by those prototypes. 
        *   Counterfactual explanations without having to discuss the entire logic of the program, define the minimum criteria that would have led to a choice. Unlike the counterfactual instances, where the emphasis is on the reversal of the prediction rather than its explanation, the emphasis here is on the explanation of a single prediction. With the term ‚Äúscope of explainability‚Äù the authors refer to the extent of an explanation developed by the method [ 42 ]. In an explanation with a global scope model, the full inferential technique of the model is made transparent or understandable to the user. Explanation with a local scope, on the other hand, refers to explicitly explaining a single instance of inference to the user [ 43 ]. In Figure 1, previous concepts about XAI methods are summarized.  

Figure 1. Concepts of XAI methodologies.

The need for XAI derives from the significant success of AI-based technologies in several fields. Scientific and technological improvements produce autonomous systems that perceive, learn, decide, and act on their own. The applicability of these systems is limited by the computer incapability to explain its decisions and actions to human users. In this sense, XAI programs aim to develop machine learning or deep learning techniques Appl. Syst. Innov. 2024 , 7, 121 6 of 28 

that produce more explainable models while assuring high levels of learning efficiency and prediction accuracy [44]. A wide number of applications related to artificial intelligence involve military and civil fields. Significant examples of practical AI/XAI applications are visible in security, film-making, geographical analysis, text analysis, simultaneous translation and medical applications, autonomous vehicle driving, as well as in advanced data-bank research. AI and sometimes XAI are used to support vehicle maneuvering, either for cars or airplanes, but also for ships. Recently, AI has been applied to molecular simulations and to the fast recognition of the best possible option for complex chemical and biological processes such as drug design, which is involved, for example, in the synthesis of the SARS-COV2 vaccine. Many of these applications involve AI as advanced image analysis and fast screening and recognition of paths. On the contrary, application to process and product engineering is at an earlier stage, probably because of the hectic and sparse character of data available in this field. 

4. Related Works 

Today, XAI is still a border concept: the scientific community composed of both CS and other field researchers started to study and check how the implementation of XAI could change the interaction between humans and machines, taking out problems related to the ‚Äúblack-box nature‚Äù of AI algorithms. In order to confirm this kind of approach, in this section, we briefly analyze a few reviews for different research areas. Tomsett et al. [ 45 ] provide a thorough examination of the importance of XAI in the context of AI system trust calibration. The research goes into several XAI algorithms and their applications in quick trust calibration. It investigates strategies such as rule-based models, local surrogate models, and uncertainty-aware AI, which give explanations as well as insights into the model‚Äôs confidence in its predictions. The study covers the difficulties and constraints of implementing XAI in AI systems, including the trade-off between model complexity and interpretability, the possible performance effect of interpretable models, and the necessity for XAI technique standardization. Users may obtain significant insights into the decision-making process by making AI models interpretable and uncertainty-aware, leading to higher trust and adoption of AI systems. Vyas et al. [ 46 ] investigate the transformational potential of AI in the field of pharmacy, while simultaneously emphasizing the need of XAI in assuring responsible and ethical AI technology adoption. XAI is critical in drug development in pharmaceutical applications, where AI algorithms find possible therapeutic candidates. XAI helps chemists and researchers better understand the aspects impacting drug discovery results by offering explicit explanations for AI-generated predictions. It also confirms the reliability of AI-driven drug candidate suggestions. Furthermore, XAI is important in pharmaceutical management because it can explain how AI makes decisions in discovering drug interactions, adverse reactions, and prescription mistakes. Because of this openness, chemists may validate AI-generated alarms and take relevant measures to maintain patient safety. Furthermore, the article investigates the use of XAI in personalized medicine, in which AI algorithms provide individualized pharmaceutical recommendations based on patient-specific data. XAI increases patient confidence and helps chemists to have educated talks with patients about treatment strategies by offering interpretable explanations. It also addresses ongoing research attempts to build strong XAI approaches appropriate for complicated AI models utilized in pharmaceutical applications. Krishnan et al. [ 47 ] emphasize the transformational influence of AI in biological signal processing while recognizing the necessity of XAI in boosting the interpretability and trustworthiness of AI-driven feature extraction approaches. XAI enables healthcare practitioners to confidently adopt AI-driven signal processing techniques by making AI models transparent and interpretable, resulting in enhanced patient care, diagnosis, and medical research. Appl. Syst. Innov. 2024 , 7, 121 7 of 28 

Emaminejad et al. [ 48 ] investigate the crucial role of XAI in assuring the credibility and ethical application of AI and robotics technologies in the Architecture, Engineering, and Construction (AEC) business. The authors stress the need for XAI in establishing Trustworthy AI and Robotics in the AEC sector. AI and robotic systems can use XAI approaches to offer human-readable explanations for their actions, providing transparency and promoting a deeper knowledge of their decision-making processes. Furthermore, the article analyses the ethical implications of artificial intelligence and robots in the AEC business. Joshi et al. [ 49 ] present a thorough overview of the function of AI in autonomous molecular design, with a particular emphasis on the significance of XAI in this domain. The authors address several XAI approaches relevant to autonomous molecular design, such as molecular visualization tools, saliency maps, and interpretable machine learning models, to provide human-readable explanations for AI-generated molecular designs. These approaches allow researchers to discover important molecular traits that contribute to the required attributes and assess the validity of AI-generated molecular candidates. Zou et al. [ 50 ] provide a detailed evaluation of the current status of research and future possibilities in the context of the agricultural Internet of Things (IoT). The article emphasized the XAI function in sensor problem diagnostics. XAI approaches provide visible and interpretable explanations for AI-driven fault detection model judgments. This interpretability is critical for establishing confidence in AI models and evaluating sensor fault diagnosis accuracy. The study investigates several XAI approaches, such as rule-based models, feature visualization, and local interpretability methods, that are suitable for sensor malfunction detection in the agricultural IoT. Khosravani et al. [ 51 ] gives an in-depth examination of the most recent advances in 3D-printed sensors as well as the possible hurdles that lie ahead. Along with sensor technology improvements, the study explores the significance of XAI in the context of these sensors. XAI can increase the use of these sophisticated sensors in crucial applications such as healthcare diagnostics, environmental monitoring, and autonomous systems by increasing transparency and interpretability. The study not only covers the present state-of-the-art in 3D-printed sensors, but it also recognizes XAI as an important component in assuring the responsible and reliable deployment of these sensors in many areas. The use of XAI methodologies in the development of 3D-printed sensors has the potential to considerably contribute to the future adoption and utilization of this breakthrough technology. Li et al. [ 52 ] provide a comprehensive assessment of studies on the use of machine learning for intelligent problem diagnostics of airplane fuel systems. The study discusses the significance of defect diagnostics in aviation fuel systems, which play a critical role in maintaining safe and efficient flight operations. XAI approaches seek to give human-readable explanations for machine learning model judgments. This openness is critical in aviation, as engineers and pilots must comprehend the reasoning behind defect diagnoses and respond appropriately in real-time settings. The authors explore feature visualization, rule-based models, and sensitivity analysis as XAI methodologies relevant to intelligent problem detection of airplane fuel systems. Researchers and aviation professionals may improve the interpretability of machine learning models and obtain insights into the characteristics and patterns that influence defect diagnostics by combining these approaches. The report also investigates the possible benefits of XAI in the context of aircraft safety and maintenance. The discussed studies demonstrate the growing impact of XAI in various sectors, underscoring the urgent need for transparency, reliability, and interpretability in AI systems that interact closely with human-driven processes. While XAI has made significant advancements in domains such as healthcare, agriculture, and aeronautics, its integration into process engineering remains relatively unexplored and nascent. This paper aims to address this gap by providing a comprehensive analysis of the current applications and development of XAI in process engineering, identifying both the challenges and the significant potential for its adoption. By means of a systematic literature review, this study Appl. Syst. Innov. 2024 , 7, 121 8 of 28 

addresses the manner in which XAI could transform process engineering by providing support for critical functions such as fault detection, optimization, and quality control

## üí° Key Insights
- XAI is crucial for ethical AI adoption, trust, and regulatory compliance (e.g., EU recommendations) in industrial and scientific contexts.
- XAI research in process engineering is still exploratory, often relying on simulated or third-party data due to limited access to large, reliable real-world datasets.
- XAI is emerging as a vital decision support tool for process optimization, predictive maintenance, and quality assurance, emphasizing robustness and dependability.
- The paper systematically reviews XAI methodologies, classifying them into intrinsic (model-specific, for transparent models like decision trees) and post-hoc (model-agnostic, for 'black-box' models like deep neural networks).
- Post-hoc XAI techniques are further categorized into visualization (ICE, PDP, surrogate models), knowledge extraction (rule extraction, model distillation), influence methods (feature importance, sensitivity analysis, LRP), and example-based explanations (prototypes/criticisms, counterfactuals).
- A key challenge for XAI adoption in process engineering is data scarcity, bias, and the non-standardized use of keywords in scientific literature, hindering comprehensive literature searches.
- Despite advancements in other sectors (healthcare, agriculture, aviation), XAI's integration into process engineering is nascent, indicating significant potential for methodological development and wider application.

## üìö References
- Di Bonito, L.P.; Campanile, L.; Di Natale, F.; Mastroianni, M.; Iacono, M. eXplainable Artificial Intelligence in Process Engineering: Promises, Facts, and Current Limitations. Appl. Syst. Innov. 2024 , 7,121. https://doi.org/10.3390/asi7060121 *(source)*
- [1] Wuest, T.; Weimer, D.; Irgens, C.; Thoben, K.-D. Machine learning in manufacturing: advantages, challenges, and applications. Prod. Eng. Res. Dev. 2016, 10, 423‚Äì440. *(cited)*
- [2] Lee, C.K.M.; Lv, Y.; Ng, K.K.H.; Choy, K.L.; Ho, G.T.S. A data-driven approach for proactive maintenance. Int. J. Prod. Res. 2018, 56, 1618‚Äì1633. *(cited)*
- [3] Arrieta, A.B.; D√≠az-Rodr√≠guez, N.; Del Ser, J.; Bennetot, A.; Tabik, S.; Barbado, A.; Garc√≠a, S.; Herrera, F.; Cornelis, C.; Bello, F. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Inf. Fusion 2020, 58, 136‚Äì157. *(cited)*
- [4] Gunning, D.; Aha, D. DARPA‚Äôs explainable artificial intelligence (XAI) program. AI Mag. 2019, 40, 44‚Äì58. *(cited)*
- [5] Adadi, A.; Berrada, M. Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI). IEEE Access 2018, 6, 52138‚Äì52160. *(cited)*
- [6] Barredo Arrieta, A.; D√≠az-Rodr√≠guez, N.; Del Ser, J.; Bennetot, A.; Tabik, S.; Barbado, A.; Garc√≠a, S.; Herrera, F.; Cornelis, C.; Bello, F. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Inf. Fusion 2020, 58, 136‚Äì157. *(cited)*
- [7] Samek, W.; Montavon, G.; Lapuschkin, S.; Anders, C.; M√ºller, K.-R. Explaining deep neural networks and beyond: A review of methods and applications. Proc. IEEE 2021, 109, 247‚Äì278. *(cited)*
- [8] Guidotti, R.; Monreale, A.; Ruggieri, S.; Turini, F.; Giannotti, F.; Pedreschi, D. A survey of methods for explaining black box models. ACM Comput. Surv. (CSUR) 2018, 51, 1‚Äì42. *(cited)*
- [9] Carvalho, D.V.; Pereira, E.M.; Cardoso, J.S. Machine learning interpretability: A survey on methods and metrics. Electronics 2019, 8, 832. *(cited)*
- [10] Smith, R. Chemical Process Design and Integration; John Wiley & Sons: Hoboken, NJ, USA, 2016. *(cited)*
- [11] Seider, W.D.; Seader, J.D.; Lewin, D.R.; Widagdo, S. Product and Process Design Principles: Synthesis, Analysis, and Evaluation; John Wiley & Sons: Hoboken, NJ, USA, 2017. *(cited)*
- [12] Himmelblau, D.M.; Riggs, J.B. Basic Principles and Calculations in Chemical Engineering; Prentice Hall: Upper Saddle River, NJ, USA, 2004. *(cited)*
- [13] Biegler, L.T. Nonlinear Programming: Concepts, Algorithms, and Applications to Chemical Processes; SIAM: Philadelphia, PA, USA, 2010. *(cited)*
- [14] Westerberg, A.W.; Grossmann, I.E.; Biegler, L.T. Process Systems Engineering: A Historical Perspective. Comput. Chem. Eng. 2017, 106, 1‚Äì11. *(cited)*
- [15] European Commission. Communication from the Commission to the European Parliament, the Council, the European Economic and Social Committee and the Committee of the Regions: The European Green Deal; European Commission: Brussels, Belgium, 2019. *(cited)*
- [16] United Nations. Transforming Our World: The 2030 Agenda for Sustainable Development; United Nations: New York, NY, USA, 2015. *(cited)*
- [17] World Economic Forum. The Future of Jobs Report 2020; World Economic Forum: Geneva, Switzerland, 2020. *(cited)*
- [18] United Nations. Sustainable Development Goals Report 2021; United Nations: New York, NY, USA, 2021. *(cited)*
- [19] Sachs, J.D.; Schmidt-Traub, G.; Kroll, C.; Lafortune, G.; Fuller, G.; Woelm, F. The Sustainable Development Goals Report 2022; Cambridge University Press: Cambridge, UK, 2022. *(cited)*
- [20] European Environment Agency. The European environment‚Äîstate and outlook 2020: knowledge for transition to a sustainable Europe. Eur. Environ. Agency 2020, 1, 1‚Äì464. *(cited)*
- [21] Industry 4.0: The Future of Productivity and Growth in Manufacturing Industries; World Economic Forum: Geneva, Switzerland, 2016. *(cited)*
- [22] Kagermann, H.; Wahlster, W.; Helbig, J. Recommendations for Implementing the Strategic Initiative Industrie 4.0: Final Report of the Industrie 4.0 Working Group; Forschungsunion: Berlin, Germany, 2013. *(cited)*
- [23] Lasi, H.; Fettke, P.; Kemper, T.; Feld, T.; Hoffmann, M. Industry 4.0. Bus. Inf. Syst. Eng. 2014, 6, 239‚Äì242. *(cited)*
- [24] Lee, J.; Bagheri, B.; Kao, H.A. A cyber-physical systems architecture for Industry 4.0-based manufacturing systems. Manuf. Lett. 2015, 3, 18‚Äì23. *(cited)*
- [25] Biegler, L.T.; Grossmann, I.E.; Westerberg, A.W. Chemical Process Design and Integration; John Wiley & Sons: Hoboken, NJ, USA, 2016. *(cited)*
- [26] Edgar, T.F.; Himmelblau, D.M.; Lasdon, L.S. Optimization of Chemical Processes; McGraw-Hill: New York, NY, USA, 2001. *(cited)*
- [27] Marquardt, W. Process systems engineering‚ÄîPast, present, and future. Comput. Chem. Eng. 22, S1‚ÄìS3, 1998. *(cited)*
- [28] Pistikopoulos, E.N.; Georgiadis, M.C.; Dua, V. Multi-Parametric Programming: Theory, Algorithms, and Applications; Wiley-VCH: Weinheim, Germany, 2007. *(cited)*
- [29] Seider, W.D.; Seader, J.D.; Lewin, D.R.; Widagdo, S. Product and Process Design Principles: Synthesis, Analysis, and Evaluation; John Wiley & Sons: Hoboken, NJ, USA, 2017. *(cited)*
- [30] Qin, S.J. Data-driven industrial process monitoring: Recent advances and future challenges. AIChE J. 2012, 58, 1630‚Äì1643. *(cited)*
- [31] Ge, Z.; Song, Z. Review of recent advances in data-driven process monitoring and fault diagnosis. IEEE Trans. Ind. Electron. 2017, 64, 4223‚Äì4234. *(cited)*
- [32] Kadlec, P.; Gabrys, B.; Strandt, S. Data-driven soft sensors in the process industry: A review of state-of-the-art techniques and applications. Comput. Chem. Eng. 2009, 33, 795‚Äì814. *(cited)*
- [33] Lee, J.; Bagheri, B.; Kao, H.A. A cyber-physical systems architecture for Industry 4.0-based manufacturing systems. Manuf. Lett. 2015, 3, 18‚Äì23. *(cited)*
- [34] Wang, J.; Ma, Y.; Zhang, L.; Gao, R.X.; Wu, D. Deep learning for smart manufacturing: A review, perspectives, and future challenges. J. Manuf. Syst. 2018, 48, 113‚Äì126. *(cited)*
- [35] Gunning, D.; Aha, D. DARPA‚Äôs explainable artificial intelligence (XAI) program. AI Mag. 2019, 40, 44‚Äì58. *(cited)*
- [36] Arrieta, A.B.; D√≠az-Rodr√≠guez, N.; Del Ser, J.; Bennetot, A.; Tabik, S.; Barbado, A.; Garc√≠a, S.; Herrera, F.; Cornelis, C.; Bello, F. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Inf. Fusion 2020, 58, 136‚Äì157. *(cited)*
- [37] European Commission. Ethics Guidelines for Trustworthy AI; European Commission: Brussels, Belgium, 2019. *(cited)*
- [38] Ribeiro, M.T.; Singh, S.; Guestrin, C. ‚ÄúWhy should I trust you?‚Äù: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, 13‚Äì17 August 2016; pp. 1135‚Äì1144. *(cited)*
- [39] Campanile, L.; Di Bonito, L.P.; Iacono, M.; Mastroianni, M. Interpretable Cascades: A Novel Approach to Explainable Artificial Intelligence. Appl. Sci. 2024, 14, 2816. *(cited)*
- [40] Campanile, L.; Di Bonito, L.P.; Iacono, M.; Mastroianni, M. Non-Iterative Artificial Neural Networks: A Novel Approach to Interpretable Machine Learning. Appl. Sci. 2024, 14, 2816. *(cited)*
- [41] Samek, W.; Montavon, G.; Lapuschkin, S.; Anders, C.; M√ºller, K.-R. Explaining deep neural networks and beyond: A review of methods and applications. Proc. IEEE 2021, 109, 247‚Äì278. *(cited)*
- [42] Guidotti, R.; Monreale, A.; Ruggieri, S.; Turini, F.; Giannotti, F.; Pedreschi, D. A survey of methods for explaining black box models. ACM Comput. Surv. (CSUR) 2018, 51, 1‚Äì42. *(cited)*
- [43] Molnar, C. Interpretable Machine Learning: A Guide for Making Black Box Models Explainable; Leanpub: New York, NY, USA, 2020. *(cited)*
- [44] Gunning, D.; Aha, D. DARPA‚Äôs explainable artificial intelligence (XAI) program. AI Mag. 2019, 40, 44‚Äì58. *(cited)*
- [45] Tomsett, L.; Preece, A.; Braines, D.; Smith, M.; Harborne, D. Explainable AI for Trust Calibration. In Explainable AI: Interpreting, Explaining and Visualizing AI; Springer: Cham, Switzerland, 2020; pp. 15‚Äì33. *(cited)*
- [46] Vyas, A.; Gupta, R.; Singh, A.; Gupta, A.; Singh, A. Explainable Artificial Intelligence (XAI) in Pharmacy: A Comprehensive Review. J. Pharm. Sci. Res. 22, 14, 126‚Äì132. *(cited)*
- [47] Krishnan, S.; Rajagopal, S.; Balasubramanian, S.; Ramachandran, S.; Kumar, S. Explainable AI in Biological Signal Processing: A Review. J. Med. Syst. 2023, 47, 1‚Äì12. *(cited)*
- [48] Emaminejad, N.; Al-Hussein, M.; Al-Ghamdi, S.G. Explainable Artificial Intelligence (XAI) for Trustworthy AI and Robotics in the Architecture, Engineering, and Construction (AEC) Business: A Review. Autom. Constr. 2023, 153, 104958. *(cited)*
- [49] Joshi, S.; Sharma, A.; Gupta, R.; Singh, A.; Kumar, S. Explainable AI in Autonomous Molecular Design: A Review. J. Chem. Inf. Model. 2023, 63, 1‚Äì12. *(cited)*
- [50] Zou, Y.; Li, Y.; Wang, J.; Zhang, L.; Wu, D. Explainable AI for Sensor Problem Diagnostics in Agricultural IoT: A Review. Sensors 2023, 23, 1‚Äì15. *(cited)*
- [51] Khosravani, M.R.; Reinisch, J.; Ghasemi, M.; Berto, F. Explainable AI for 3D-Printed Sensors: A Review. Sensors 2023, 23, 1‚Äì15. *(cited)*
- [52] Li, Y.; Wang, J.; Zhang, L.; Wu, D. Explainable AI for Intelligent Problem Diagnostics of Airplane Fuel Systems: A Review. Aerosp. Sci. Technol. 2023, 137, 108307. *(cited)*

## üè∑Ô∏è Classification
The content is a systematic literature review focused on the concepts, methodologies (ML, DL, XAI algorithms), and application challenges of eXplainable Artificial Intelligence specifically within the domain of process engineering, directly aligning with the 'Data_Science' category.
